[Home](https://clojia.github.io/) | [Research](https://clojia.github.io/research/) | [Last](https://clojia.github.io/research/2018-08-IR-Dist-Rep) | [Next](https://clojia.github.io/research/2018-09-IR-GANs)

## Index
F.A. Gers, J. Schmidhuber, and F. Cummins. Learning to forget: continual
prediction with LSTM. Neural Computation, 12(10):2451â€“2471,
2000.

## Motivation
Although previous Long Short-Term Memory (LSTM) model outperformed traditional Recurrent Neural Network (RNN) in many areas. It still has limitation in "processing continual input streams withouth explicit marked sequence ends". This work inctroduced *forget gate*, which "enables an LSTM cell to learn to reset itself at appropriate times" to solve the problem.

## Approach
Based on "standard" LSTM then, which included input gate, output gate and memory cell, the activations look like:
- Inpute gate:
<img src="../images/in_gate.png" width="300"> 

- Output gate:
<img src="../images/out_gate.png" width="300"> 

- Memory cell:
<img src="../images/cell.png" width="300"> 

The paper proposed extended LSTM, which uses adaptive forget gate to "reset memory blocks once their contents are out of date". Simply by replacing "Constant Error Carousel" (CEC, 1.0) by the multiplicative forget gate activation:

<img src="../images/forget_gate.png" width="300"> 

Thus the memory cell activation turns out to be:

<img src="../images/cell_new.png" width="300"> 

The backpropagation process could be found in [previous note](https://clojia.github.io/research/2018-08-IR-RNN-BP).

The proposal was tested on "embedded Reber grammar" (ERG) benchmark problem: the second and penultimate symbols are always the same, and std LSTM outperformed other RNN algorithms. Extended LSTMs with forget gates were tested on continual variant of the ERG problem, which does not ""provide information about the beginnings and ends of symbol strings", and extended LSTM with learning rate decay achieved the best results.

## Limitation 
