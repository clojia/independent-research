[Home](https://clojia.github.io/) | [Research](https://clojia.github.io/research/) | [Last](https://clojia.github.io/research/2018-08-IR-Open-Set-Recognition) | [Next](https://clojia.github.io/research/2018-08-IR-Open-Max)

## Index

M. T. Ribeiro, S. Singh, and C. Guestrin. "Why Should I Trust You?":
Explaining the Predictions of Any Classifier. In SIGKDD, 2016

[github](https://gist.github.com/shagunsodhani/bd744ab6c17a2289ca139ea586d1d65e)

## Motivation
As machine learning techniques are rapidly developed these days, there are plenty of models remain mostly black box. The paper is to make the predictions more interpretable to non-expertises despite which model it is (model-agnostic), so that people can make better decisions  (LIME). Also, expertise can choose better classifiers as well as instances (SP-LIME), to achieve better model performance.

## Approach

The paper proposed a method - Local Interpretable Model-agnostic Explainations (**LIME**) to "identify an interpretable model over interpretable presentation that is locally faithful to the classifer. It introduced sparse line explanations, weighing similarity between instance and its interpretable version with their distance. 

The paper also suggested a Submodular pick algorithm (**SP-LIME**) to better select instances by picking the most important features based on the explanation matrix learnt in **LIME**.

The methods were associated with different models, tested on both image data and text data to learn explanations, and the result shows that the explanations could get us insights into "classifers knowing when not to trust them and why".


## Limitation 
The paper mainly focuses on linear explanations, maybe could achieve better performance with non-linear ones. Or even using different ones with different decision-making models, which should under the prerequisite that the model is not black-box.
