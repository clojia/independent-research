[Home](https://clojia.github.io/) | [Independent Research](https://clojia.github.io/independent-research/) 

## Index
Tang, Jian, et al. "Line: Large-scale information network embedding." Proceedings of the 24th international conference on world wide web. International World Wide Web Conferences Steering Committee, 2015.

## Motivation
The paper proposed a novel network embedding method which suites both directed/undirected and/or weighted networks, it also proposed a propose an edge-sampling algorithm for optimizing the objective.

## Method
<img src="images/line-example.png" width="300"> 
LINE explores both first-order and second-order proximity. First-order proximity is connected through a a tie, such as node 5 and 7(aka. neighbours). Second-order proximity nodes are not directly connected with each other, however they should
also be placed closely as they share similar neighbors such as node 5 and 6.

##### First-order proximity
Firt-order proximity is defined by joint probability between two nodes v_i, v_j:

<img src="images/line-1p.png" width="200"> 

where u is the low-dimensional vector representation. And the empirical probability can be defined as <img src="images/line-emp-1.png" width="100">, where W is the summation of all the weights in the graph. To preserve the first-order proximity.

The objective function looks like:

<img src="images/line-1od.png" width="200"> 

Using KL-divergence as distance function, the objective function becomes:

<img src="images/line-1o.png" width="200"> 

##### Second-order proximity
Second-order proximity assumes that vertices sharing many connections to other vertices are similar to each other. For each directed edge (i, j),
Line defines the probability of “context” v_j generated by vertex v_i as:

<img src="images/line-2p.png" width="200"> 

where |V| is the number of vertices or “contexts”. And the empirical probability can be defined as <img src="images/line-emp-2.png" width="100">, where d_i is the out-degree of vertex i.

The objective function of second-order proximity looks like:

<img src="images/line-2od.png" width="200"> 

Using KL-divergence as distance function, the objective function becomes:

<img src="images/line-2o.png" width="200"> 

##### Negative sampling
Line also adopts negative sampling method to optimize objective functions. It samples multiple negative edges according to some noisy distribution for each edge (i, j) using the following objective function:

<img src="images/line-neg-sampling-obj.png" width="200"> 

The first term models the observed edges, the second term models the negative edges drawn from the noise distribution and K is the number of negative edges.
